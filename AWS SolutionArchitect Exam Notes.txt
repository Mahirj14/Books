Exam questions:
================

Domain 1: Design Secure Architectures 30%     ---> [IAM], AWS Single Sign-On [AWS SSO], MFA,AZ,Regions,Security 						   practices,AWS Control Tower,Service control policies,Data access and governance, Data recovery, Encryption, KMS

	
Domain 2: Design Resilient Architectures 26%  --->
Domain 3: Design High-Performing Architectures 24% --> S3,EFS,EBS,ECS,EKS,DB
Domain 4: Design Cost-Optimized Architectures 20%


IAM
====

=> Root account created by default.shouldn't be used or shared

Users --> Users are people within the org and can be grouped. Users don't have belong to a grp or they can belong to multiple grp 

Groups --> Only contain users, not other gps

=> Global service and allows to manage multiple accounts. One account is management account and other accounts are member account

Policies
========

Inline policy --> If User doesnt belongs to any group and if they want to have access to access any other services we can achieve that using inline policy
   		  
Service control policy --> SCP enables central administration over the permissions available within the accounts in our organisation

Backup policy, Tag policy

IAM Conditions
===============

(i) Source ip --> This is used to restric client IP from which API calls. Unless the client makes an API call from within the required ip addresses then API call being denied.

(i.e) You are using company n/w, if you sorce ip it guranteeing that only your company can access your own aws env.

(ii) AWS requested region --> Here we deny the resource access to the specific region.

=> aws:principleorgid can be used in any resource policy to restrict access to the accounts that are memeber of organisation

IAM policy consist of below items
=================================

version: Version # and always include "2012/10/17" which policy language version

Id: Identifier of the policy (optional)

Statement: one or more individaula statement and statement consist of 

Sid: Identifier of the statement (optional)
Effect: whether allow or deny the access
Principal: which account,user or role to which this policy applied to.
Action: List of API calls that either denied or allowed based on the effect
Resource: list of resources to which action applied to

(i) IAM permission boundaries --> we can create a user and can give full access to that user but we can control that full access for specfic resource via setting up permission boundaries. 

(i.e) Smith is an user and he has admin access but we can set permission boundary for s3 like he can have full access only for s3 not for other resources.

MFA
====

(i) AWS Management console
(ii) AWS Coomand Line Interface(CLI) protected by access key
(iii) AWS s/w Developer kit(SDK)(access and manage aws services programatically) 
      It supports javascript,python,php,.net,ruby,java,go,nodejs,c++
      Mobile SDK: Android,IOS

Cmd: To list out iam users --> aws iam list-users

IAM Roles for services
=======================

=> Some aws service need to perform actions on our behalf ,to do so we will assign permissions to aws service with IAM role

Roles:
EC2 instance role,Lambda function role,Roles for CF

IAM Security tools
==================

(i) IAM credential report(account-level)--> This report lists all our account users' and the status for their cred

(ii) IAM Access Advisor(user-level)--> Shows the service permission granted to a user and when those service last accessed0 also we can use this to revise policies.
                                       

IAM Active directory(AD)
=========================

=> (i.e) Jhon is a user and that user has domain controller and under the domain controller few machines are connected. No need login all the machines simultaniously once we connect Domain controller from there we can connect all the sub machines. This is the idea behind AD.

(i) AWS managed microsoft AD --> we can create our own AD in aws and here we can manage users locally also it supports MFA.

=> Establish trust connection with on-prem AD

(ii) AD connector

=> Directory Gateway(proxy) to redirect to on-prem AD, supports MFA

=> Users are managed on the on-prem AD

(iii) Simple AD --> AD-Compatible managed directiory on AWS and cannot be joined with on-prem AD

AWS Control Tower
===================

=> Easy way to setup and govern secure and complain multiple accounts

=> Benefits to automate ongiong policy mgnt using guardrails

=> Detect policy violation and remediate them

=> Monitor compliance through interactive dashboard

What is guardrails?

=> We can setup multiple accounts but if want to restrict all of them at once for certain kind of things or monitor the compliance for certain kind things we can do that using guardrails within control tower env.

* Preventive guardrail --> using scps(restrict region across all ur accounts)

(i.e) We could create preventive guardrail to restrict regions across all ur accouts to say should only operate us-east-1 in the eu-west-2 regions

* Detective guardrail --> AWS config(identity untagged resources)

(i.E) we want to identify untagged resources in our accoutns we can setup detective guardrail and so its going to use config and config is going to deployed on all our member accounts. And this will monitor untagged resources, if anything find out it will trigger the SNS topic then admin will know abt this

EC2(Elastic Compute Cloud) --> 
==========================

=> Ec2 is Infrastructure as service.It consist of Renting VM(EC2),Storing Data on virtual drives(EBS),Distributing loads across machines(ELB),scaling the services(ASG)

EC2 User data
==============

=> It is posible to bootstrap our instance using an Ec2 user data script. Bootstrap means launching cmds when machine starts

=> script is only run once at the instance first start. 

=> Ec2 user data used to automate tasks like installing updates,installing s/w, downloading common files from the internet.

=> Ec2 user data script runs with the root user

Security Group
==============

=> SG can be attached to multiple instances. but best practice is maintain seperate sg for ssh access.

(i) SSH Troubleshooting --> connection timeout

=> This is a security group issue. Ensure your security group assigned correctly to instance

=> security group is properly configured and still a connection timeout issue. This means a corporate firewall or a personal firewall is blocking the connection

(ii) connection refused --> 

=> This means the instance is reachable, but no SSH utility is running on the instance

Try to restart the instance If it doesn't work, terminate the instance and create a new one. Make sure you're using Amazon Linux 2

(iii) Permission denied

=> using the wrong security key. look at your EC2 instance configuration to make sure you have assigned the correct key

=> using the wrong user. Make sure you have started an Amazon Linux 2 EC2 instance, and make sure you're using ec2-user

EC2 instance Types
===================

(i) General Purpose --> Suitable for diversity of workloads such as webserver or repo. Provide blz b/w     				Compute,Memory, Networking

(ii) Compute optimized --> Great for compute-intensive tasks requires high performance processors like 
			   Batch processing workloads, Media transcoding,High performance webserver,High  				   performance computing,scientific modeling and Machine learning,gaming server

(iii) Memory optimized --> Fast performance for workloads that process large datasets in memeory
			   High performance, relational/non-relational db,Distributed web scale cache stores
   			   Business intelligence,Application performance real-time processing of big   					   unstructured data

(iv) Storage optimized --> Great to access large data sets on local storage
			   High frequency online transaction processing(OLTP) s/y, Relational& Nosql db,
			   Cache for in-memory db like Redis,Data warehouse, Distributed file s/y

EC2 instance purchasing
=======================

On-Demand --> Pay for what you use. Linux/windows billing per second after the first minute
              All other OS billing per hr.

	  --> Recommended for short term and un-interrupted workloads where you can't predict how application 		      will behave

Reserved --> 72% discount compared to on-demand. Can reserve specific instance attributes lke instance 			     type,region,tenacy,OS

	--> Reservation period 1yr+dis or 3 yrs more dis

	--> Recommended for steady state application usage. Its a convertible instance we can change instance  		    type,family,os and tenancy

Spot --> Most cost-effective and useful for workloads that are resilent to failure like batch jobs,data analyis, img processing but not suitable for critical jobs or databases.

=> To terminate spot instane first we need to cancel spot request that or open,active or disabled then terminate the associated spot instance


IP Addreses
===========

Public ip --> Servers can talk to each other using public ip

=> When use public ip you are accesible over the internet when you use private ip only accessible within private ip n/w.

Private ip -->  two diff private networks(two companies can hav same range private ip	
       		
		When we are using ssh we can't use private ip bcoz we are not in the same n/w

Elastic IP --> Eip is a public ipv4 ip and we can attach it one instance at a time. We can have max 5 eip for an account
               If you want increase eip count need to ask AWS

Placement groups
================

=> You can use placement groups to influence the placement of a group of interdependent instances 

=> We can control over the ec2 instance in aws infra using placement gp strategy. In this we don't have to directly interact with h/w

3 strategies:
============

(i) Cluster --> packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of high-performance computing (HPC) applications.

=> It is suitable for Big data job that needs to comple fast and application needs extremly low latency and high n/w throughput

(ii) spread --> Across underlying h/w. max 7 instances per az, used for critical application

=> All instances are located in diff azs. So reduce risk failure. but the thing is only have limited to 7 instances per az

=> strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.

(iii) partition --> Span across many diff partitions within a az in the same region. can have 7 partitions per az and each partition can have max 100 instance

=> it will not share racks in the other partions like partition goes not down will not affect another partition

=> This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.

EC2 Hibernate
==============

=> In hibernate state Ram will be preserved like in ec2 instance root EBS volume will be encrypted and data is running in RAM When we start hibernation process instance will go in stopping state and RAM is going to dumped into EBS volume. 

=> When instance shutdown RAM disappers but EBS volume still contains dump of the RAM. When instance is started RAM is loaded from the disk onto the ec2 instance memory

=> Instance RAM size must be less than 150GB and instance cannot be hibernated more than 60 days

=> it supports all kind instances and AMI's but root volume must be encrypted it should not be instance storage

EBS(elastic block Storage)
==========================

=> It allows instance to persist data even after their termination.

=> They can be only mounted to one instance at a time. EBS volume is n/w drive not a physical drive. It can detached from instance and attach it to other instance

=> EBS volume can lock to an az.if you do so like if ebs locked into us-east-1a cannot used to us-east-1b,so to move across volume you should take snapshot first.

=> we can increase the capacity of volume

=> by defualt attached ebs volume is not deleted so need to enable "Delete on termination" while creating it. If you want to preserve data when instance is terminated need to disable this option.

=> When creating EC2 instances, you can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard).

=> Using EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ. Each EC2 instance has full read/write permissions

=> EBS snapshot archive tier is 75% cheaper but it takes 24-75hrs for restoring archive

=> We can setup rules to retain deleted snapshots. can speicy retention period from 1 day to 1yr

=> Fast snapshot restore has no latency but it is expensive

=> We can encrypt ebs volumue using KMS. Snapshots of encrypted volumes are encrypted

EFS(Elastic File System)
========================

=> EFS is shared and network file s/y

=> Managed NFS(n/w file sys) can be mounted on many EC2 and these instance also can be in diff azs.

=> Highly available,scalable,expensive(3xgp2), pay per use

=> Posix fs(linux) has std file api

=> EFS is a network file system (NFS) that allows you to mount the same file system on EC2 instances that are in different AZs.

=> EFS is a network file system (NFS) that allows you to mount the same file system to 100s of EC2 instances. Storing software updates on an EFS allows each EC2 instance to access them.

=> Amazon EFS is not supported on Windows instances.

AMI
===

=> AMIs are built for a specific AWS Region, they're unique for each AWS Region. You can't launch an EC2 instance using an AMI in another AWS Region, but you can copy the AMI to the target AWS Region and then use it to create your EC2 instances.

ELB & ASG
==========

=> Vertical scalability means increasing the size of the instance. It is common for non-distributed s/y such as db.

=> Horizontal scalability means increasing the number of instances/systems for ur application. It is common for web applications

=> Elb/Alb provides a static DNS name but it does NOT provide a static IP. The reason being that AWS wants your Elastic Load Balancer to be accessible using a static endpoint, even if the underlying infrastructure that AWS manages changes.

=> Network Load Balancer provides both static DNS name and static IP

ALB --> load balancng to multuple http applications across target gps and also load balancing to multiple app on the same machine. (ex:cntainer)

=> Routing tbls to diff target gps based on path in url and hostname in url and can't attach an Elastic IP address to Application Load Balancers.It has a static DNS name

=> ALB great for microservice and cotainer-based app

=> ALBs can route traffic to different Target Groups based on URL Path, Hostname, HTTP Headers, and Query Strings.

=> The following cookie names are reserved by the ELB (AWSALB, AWSALBAPP, AWSALBTG).

NLB
===

NLB--> Handles millions of req per sec. NLB has 1static ip per az and supports assinging eip and not used in free tier instance.

=> Network Load Balancer provides the highest performance and lowest latency if your application needs it.

GLB--> 3rd party lb used for firewall,intrusion detection and prevention...

Cross-zone load balancing --> can evenly distributes traffic to each instaces with diff az which is configured in lb.

=> by default this option is enabled in alb(we can disable it in Target gp level) and no charges for inter az data.

Sticky session --> ELB Sticky Session feature ensures traffic for the same client is always redirected to the same target (e.g., EC2 instance). This helps that the client does not lose his session data.

sticky session feature (also known as session affinity) to enable the load balancer to bind a user's session to a specific target. This ensures that all requests from the user during the session are sent to the same target. This feature is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the client must support cookies.

SSL/TLS
========

SSL certs are used to allows traffic b/w client and lb to be encrypted.and it has expiry date.

SNI --> Server Name indication solves load multiple  ssl certs onto 1 web server for that server servs multiple 	websites.It only works ALB, NLB and cloudfront
 
    --> Server Name Indication (SNI) allows you to expose multiple HTTPS applications each with its own SSL 	certificate on the same listener

ASG
====

ASG --> Default cooling down period is 300 secs. During cooldown period ASG will not launch or terminate additional instances.

(i) Dynamic scaling policy --> Target tracking, Step scaling and Schedule action or simple scaling
(ii) Predictive scaling policy

=> You can configure the Auto Scaling Group to determine the EC2 instances' health based on Application Load Balancer Health Checks instead of EC2 Status Checks (default). When an EC2 instance fails the ALB Health Checks, it is marked unhealthy and will be terminated while the ASG launches a new EC2 instance.

RDS    
====

=> RDS managed postgressql/Aurora/Mysql/sql server/custom/Mariadb

=> RDS instance provisioning instance size,EBS vloume type &Size

=> RDS helps you store relational datasets, with SQL language compatibility and the capability of processing transactions such as insert, update, and delete

=> Supports Autoscaling for storage also supports read replicas so if you have application that need to run analytics agaist production it is much better to create rr.

=> supports multi az for high availability to have standby databases.	

=> Adv of using RDS instead deploying db on EC2 instance

* Automated provisioning, OS patching
* continuous backup and restore to specific timestamp
* Monitoring dashboards and read replicas for improved read performace
* Multi az setup for DR
* Maintenance windows for updates
* scaling capabilities
* storage backed by EBS(gp2 or io1)

=> Since RDs is managed service by AWS we cannot connct rds instances using ssh

=> if you enable RDS storage autoscaling RDS will detect and scale automatically the storage

=> We can create 15 read replicas in same az,cross az or cross region

=> We can create multiple read replicas with async replication with with main RDS db instance so reads are eventually consistent

=> Read Replicas add new endpoints with their own DNS name. We need to change our application to reference them individually to balance the read load.

=> Replicas can be promoted to their own db

=> application must update the connection string to leverage read replica

=> Usecase: RDS db created only for prod but new team wants to do monitoring report if you use sae db it will affect prod performance. so need to cread read replica with async mode.It will read reports without disturbing prod.

=> read replica only perform Select(=read) not for insert,update or delete

=> Usually AWS will charge to use service one region to another region but in RDS its exception

=> RDS multi az is used for DR it performs sychronus replication like will replicate every single changes in master db

=> We can set up read replicas as multi az for DR.

=> How do we make RDS db go from single az to multi az?

* Its a zero down time operation means no need to stop the db to go from single az to multi az. Only things is just click on modify for the db and enable multi az

* Snapshot is taken and new db restored from ss in a new az

* Synchronization established b/w two dbs so then stand by db will catch up all datas from master db

=> We can enable deletion protection to protect db from being accidental deletion

Dynamo DB
==========

=> Amazon DynamoDB is a key-value, document, NoSQL database.

=> Amazon DynamoDB can not be used to store big objects. The maximum item size in DynamoDB is 400KB.

RDS vs RDS Custom
==================

RDS: Entire db and os to be managed by AWS  and it automates setup,operation,scaling of db

RDS custom: Full admin access to the underlying os an db like we need to do config settings, install patches,etc..

=> De-activate automate mode to perform customization activities but before that need to take snapshot

Amazon Aurora
==============

=> Maaged by AWS not opensource. Serverless,on-deamnd,autoscaling config,automatically start/stop when application need

=> postgres and mysql both supported by aurora. It features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across 3 AZs.

=> Aurora  is cloud optimized and claims 5x performance improvement over mysql on RDs and 3x performance improvement over postgres

=> Aurora storage automatically increse of 10 GB upto 128TB

=> It can have upt 15 replicas and the replication is faster than  mysql and its high availability native.

=> 6 copies of 	your data across 3 azs.

=> self healing and peer-peer replication . storage is striped across 100s of volumes.

=> 1 aurora instance(Master) can take writes and if the master doesn't work automated failover in less than 30 secs.

=> on top master we can have 15 read replicas all serving reads and ay of the read replicas can become master if master fails

=> Support cross region replication 

=> Shared storag volume(Replication + Self healing + Auto expanding)

Aurora as Cluster
=================

=> We have shared storage volume which is have 10 GB to 128GB. Master is the only thing we can write to the storage when master fails Aurora provides writer endpoint(it a DNS name) and always pointing to the master.

=> If master fails still clients can talks to writer endpoint and erdicticst to right instance.

= If we hve autoscaling for read replicas it really difficult to track of where they are,wats the url and how do we connect. So reader endpoint comes into picture.

=> Reader endpoint helps to connecting load balancing and it connects automatically connect read replicas

Features of Aurora
===================

* Automatic failover, backup and recovery, Isolation and security

* Industry compliance, push-button scaling

* automated patching with 0 downtime, advance monitoring, routine maintenance

* Enable Backtrack:Restore data at any point of time without using backup

* Aurora cross region replica is recommended for DR

Aurora scaling
===============

=> If we have one writer endpoint and two reader endpoint if reqs increase aurora scaling wll create replicas

Aurora Multimaster
===================

=> If you want continuous write availability for the writer nodes we can go for aurora multi-master. In this every nodes does R/W operation and promoting read replicas to new master node

Aurora custom endpoints
=========================

=> we can run analytical query on specific replica

=> Read Replicas will help as your analytics application can now perform queries against it, and these queries won't impact the main production RDS database.

=> Reader endpoints not used after defining custom endpoints

Aurora DB cloning
==================

=> Creating a new db cluster from existing one. Its quiet faster than snapshot & restore

=> use the same volume as original db if any updates happen in original db additional storage will be allocated and data will be copied to be seperate

RDS & Aurora security
======================

=> Should be encrypt master and replicas using KMS and it should be happend during launch time

=> If the master is not encrypted and read replicas will not be encrypted

=> To encrypt unencrypted db, take db snapshot and restore as encrypted

In-flight encryption: Use TLS root certificate from client-side

IAM authentication: use IAM roles to connect db instead using username&pwd

SG: Control n/w access to RDS/ Aurora

No-SSH avialble except RDS custom

=> Enable Audit logs to send data to cloud watch logs for longer retention

Aurora serverless
==================

=> Used for unpredicatable/intermitent workloads,no capacity planning

Aurora Global: It gives 16 db read instances for each regions and the storage replication happens < 1sec. 

	       In case anything problem in primary region we can promote secondary region to become new primary 		       region Amazon Aurora is a relational, SQL-based database.

Document DB
===========

=> Document DB is same as Mongo db Nosql which is used store,query and index json data and similar deplyoment concept as Aurora.

=> Fully managed and HA in 3 az. Document db storage automatically grows in 10gb to 64tb

=> Provide millisec latency responsive

Amazon keyspace(Casandra for apache)
=======================================

=> open-src nosql distributed db and automatically scales up/down based on traffic

=> tbls are 3 times replicated to multi-az. Single digit latency,1000s of recors per sec

=> Encryption, bkup,point in time recovery upto 35 days

QLDB(Quantum ledger db)
=======================

=> Recording financial transaction,used to review history of all the changes made to application over the time.

=> Immutable s/y means no records can be removed/modified. cryptografically verifiable.

=> No decentralized component

Amazon Neptune
==============

=> Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets.

=> As a Solutions Architect, a startup company asked you for help as they are working on an architecture for a social media website where users can be friends with each other, and like each other's posts. The company plan on performing some complicated queries such as "What are the number of likes on the posts that have been posted by the friends of Mike

Elastic cache
===============

=> 										

=> Helps to reduce load off and makes application stateless

=> If application queires elasticache if not availble get from rds and store it in Elastic cache. So if same query hits nxt time we can get result from cache. This helps to releive load in RDS

=> Elastic cache has primary and reader endpoint

=> Elasticache managed by Redis/Memcache

Redis vs Memcache
==================

=> Redis supports multi az with auto failover, Data durability using AOF persistence

=> High availability, Backup and restore features

=> Supports sets and sorted sets (Gameing platform)

Memchache:

=> Multi node for partitioning of data(sharding)

=> No high availablity, no persistent, no backup and restore

=> Multi-thread archive

RDS Backup
===========

Aurora Automated backup: --> Daily full backup. Transaction logs are backed up every 5 mins and 
                             ability to restore to any point in time

			 --> 1 to 35 days is the retention period (but cannot be disabled)

Manual DB snapshot: --> manually triggered by the user. we can have the backup as long as we want.

on-demand backup: 

You need to store long-term backups for your Aurora database for disaster recovery and audit purposes. What do you recommend --> Perform on-demand backup

Important ports
================

FTP: 21

SSH: 22

SFTP: 22 (same as SSH)

HTTP: 80

HTTPS: 443

RDS Databases ports
==================

PostgreSQL: 5432

MySQL: 3306

Oracle RDS: 1521

MSSQL Server: 1433

MariaDB: 3306 (same as MySQL)

Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)

S3
===

=> S3 is used for storage andDR, ARchive, Hybrid cloud stroage, Application hosting,Media hosting, s/w updates,
   Data lakes and big data analysis, static websites

=> S3 is a global service but Buckets are not global it should define in region level. Files in buckets are called objects

=> There is no concept of directories in bucket

=> Max object size is 5TB(5000GB). If you are uploading more than 5gb must be use multi-part upload

Bucket Policy
=============

User based:
IAM policy: --> API calls should be allowed for specific user from IAM

Resource Based:

Bucket policy --> bucket wide rules we can directly allow from s3 console across all account

=> ACLs disabled (recommended)
All objects in this bucket are owned by this account. Access to this bucket and its objects is specified using only policies.

=> ACLs enabled
Objects in this bucket can be owned by other AWS accounts. Access to this bucket and its objects can be specified using ACLs.

S3 Encryption
===================

1. Server-side Encryption: --> encryption type is AES-256
----------------------------------------------------------

(i)Server-side encryption with Amazon S3 managed keys (SSE-S3) --> Default Enccryption

=> With SSE-S3, the encryption happens in AWS and you don't manage the encryption keys. Encryption keys stored in AWS.

(ii) Server-side encryption with AWS Key Management Service keys (SSE-KMS) 

=> Using an S3 Bucket Key for SSE-KMS reduces encryption costs by lowering calls to AWS kms. S3 Bucket Keys aren't supported for DSSE-KMS

=> If you go with SSE-KMS you may be impacted by KMS limits, when you upload it calls the GenerateDatakey KMS API and when you download, it calls the Decrypt KMS Api

=> KMS quota per sc 5500,10000,30000 based on region and we can req a quota increase using the service quotas console

=> With SSE-KMS, the encryption happens in AWS, and the encryption keys are managed by AWS but you have full control over the rotation policy of the encryption key. Encryption keys stored in AWS.

(iii) Server-side encryption with customer provided keys(sse-c)

=> We can manage our own encryption keys. Fully managed by the customer outside of AWS. With SSE-C, the encryption happens in AWS and you have full control over the encryption keys.

=> Amazon s3 does not store the encryption key. To transmit key into s3 we must use HTTPS and we must pass the key as part of HTTPS headers for every req being made

(iv) Dual-layer server-side encryption with AWS Key Management Service keys (DSSE-KMS)

=> To enable replication must enable versioning and it doesn't support chaining/order like (supports a-c, a-b)

2. Client-side Encryption:
---------------------------

=> use client libraries such as amazon s3 client-side encyption library. Clients must encrypt data themselves before sending to Amazon s3.

=> Decrypt data themselves when retriving from s3.

3.Encryption in transit(SSL/TLS):
----------------------------------

=> Encryption in flight also called SSL/TLS means anytime you can visit a website and you can see green lock/lock meaning the connection b/w you and targt server is secure and fully encrypted

=> S3 exposes two endpoints 

HTTP --> Not Encrypted
HTTPS --> Encryption in flight

4. S3- Force Encryption in transit aws:SecureTransport:
-------------------------------------------------------

=> We can attach bucket policy to s3 bucket and attach statment like to deny any Getobject operation we can set it as " aws:SecureTransport:false"

=> securetransport is gonna be true whenever using HTTPS

S3 Storage classes
===================

* S3 standard --> General purpose ---> 99.99% availability, frequently accessed data,low latncy(Big data analytics,mobile,gaming and content distribution)

* S3 std infrequent access(IA) ---> DR, backup

* s3 std one-zone-infrequent access ---> Single az,data lost when az is destroyed(stroing 2ndry backup of on-prem data or data recreated)

* " glacier instant retrival ---> meant for archiving/backup, price for storage + pay for retrieval, millisec retrieval, stroage duration is 90 days

* " " flexible retrival --> (expedite- 1to5 mins,std-3 to 5 hr,bulk-5 to 12 hr)

* " " Deep Archive --> long trem storage(std-12hr,bulk-48hr),min storage is 180days

* " intelligent tiering --> small monthly monitoring ,auto-tiering free

S3 Lifecycle rules
===================

(i) Transition

=> Move objects to std IA class 60 days after creation
=> Move to glacier for archiving after 6 months

(ii) Expiring action

=> log files can be set to delete after 365 days
=> we can delete incomplete multi-part uploads as well as can delete old versions(if versioning enabled)

s3 performance
===============

=> s3 automatically scales to high req rates,latency 100-200 ms

=> Transfer accelaration: Increase transfer speed by transferring file to an edge location which will frwd the file to target location. so transfer acceleration is the grt way to speed up the transfer

File in usa --> Edge location usa ---> S3 bucket in australia

Byte range fetches: Parlelly Gets by requesting speific byte ranges

s3 Batch operation
===================

=> Perform bulk operations on existing s3 with a single req like 

* Modify object metdata & properties, Copy objs b/w s3 bucket

* Encrypt, un-encrypt objects, Modify ACLs,tags

* Restore objs from s3 glacier,Invoke lambda funs to perform custom actions on each objs

=> S3 batch operations manage retries,track progress, send completion notifications, generate reports

=> S3 inventory is uses to get obj list and S3 select uses for filter objs.

S3 Pre-Signed URL: 
===================
S3 Pre-Signed URLs are temporary URLs that you generate to grant time-limited access to some actions in your S3 bucket.

CORS - Cross Origin Resource sharing
=====================================

=> CORS is a web browser security that allows you to enble images,assets or files being retrived from one s3 bucket incase the req is originating from another region.
 
S3 Glacier vault lock
======================

=> WORM (Write once Read Many model).

=> (i.e) Take an obj and put it into glacier vault and then lock it, so it cannot be ever modified or deleted by anyone. This integrate valut lock policy

=> It helpful for compliance and data retention

Object Lock
===========

=> Object Lock works only in versioned buckets. Enabling Object Lock automatically enables Versioning.

=> Store objects using a write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. Object Lock works only in versioned buckets

=> If you enable object lock Permanently allows objects in this bucket to be locked. 

=> Enable Object Lock only if you need to prevent objects from being deleted to have data integrity and regulatory compliance. After you enable this feature, anyone with the appropriate permissions can put immutable objects in the bucket.  You might be blocked from deleting the objects and the bucket. Additional Object Lock configuration is required in bucket details after bucket creation to protect objects in this bucket from being deleted or overwritten

Retentio Mode: Compliance

=> Obj version cant be overwritten or deleted by any user,including root user and retention period cant be shortend.

Retentio Mode: Governance

=> Some users haev spl permission to change retention or delete the obj.

Legal hold:

=> protect the obj indefinitely,independently from retention period and it can be freely placed and removed using the s3:Putobjectlegalhold IAM permission.

S3 Access Points
=================

=> Access points simplyfy security mgnt for s3 buckets. Each access point has its own DNS name

=> We must create vpc endpoint to access the access point(Gateway or interface) and it only accessible within the vpc

AWS Cloudfront --> Content Delivery Network
============================================

=> Improves read performance, content is cahed at the edge of the application. 

=> 216 ege location and aws keeps adding location to improve user experience

=> In CDN we will get DDOS protection(sort of attack),since our app is a worldwide we neet to protect against , this kind of attack also using sheilds and firewalls

=> Cloudfront can be used as a ingress(to upload files in s3)

Diff b/w cloudfront and S3 replication:
=========================================

Cloudfront:

=> In Cloudfront we are using  global edge n/w and files are going to be cached in each edge location. 

=> Great for static content that must be avilable everywhere in the world

S3 Replication:

=> Must be setup for each region you want replication to happen

=> Its Read-only

=> Great for dynamic content that needs to be available at low-latency in few regions

=> If you want to access EC2 instance by using edge location instances must be in public coz there is no private VPC connectivity in cloudfront.

=> If you use ALB again it must be public and then backend ec2 intances can be private coz there is a private vpc connection b/w ALB and EC2 instance but we need modify SG to allow ALB

=> We can control the restriction of distribution in cloud front using geo restriction

AWS Snow family
=================

=> Offline devices to perform data migration if it takes more than a week to transfer over n/w use snowball devices

=> Highly secure,portable devices to collect and process data at the edge and migrate data into and outof AWS

Data migration: Snowcone, Snowball Edge(data transfer), Snowmobile

Edge computing: Snowcone, Snowball Edge

2 Types of snowball edges: snowball edge storage optimized,snowball edge compute optimized

Usecase: DR,large data cloud migration, DC decommision

=> Snowball cannot import to Glacier directly, we must use s3 first in combination with s3 life cycle policy

Amazon FSx
===========

=> Its a 3rd party high-performance file s/y on AWS

1. FSx for windows file server --> It supports SMB protocal & windows NTFS, can be configured in Multi az for HA    				and data is backed up daily to s3 for DR

2. FSx for Lustre --> Lustre is parallel ditributed fs for large scale computing

Usecases: ML,Video processing,Financial modeling, eletronic design automation

Seamless integration with s3: 

(i) can "read s3" as a fs (through fsx)
(ii) can write the o/p of computations back to s3 (through fsx)
(ii) can be used from on-prem servers(vpn or direct connect)

3. FSx file system Deployment options

Scratch file system:

=> Temp storage, data is not replicated if the server fails, high burst

Usage: short-term processing, optimize cost

Persistent file system:

=> Long-term storage, data is replicated within the same az, replaced fialed files within a min

Usage: long-term processing, sensitive data

4. FSx for Netapp ontap --> Fs compatible with NFS,SMB,iSCSI protocol, Move workloads on ontap or NAS to aws
			    storage shrink or grow automatically,snapshot replication,low-cost,compression and 				    data de-duplication, point in time cloning(helpful for testing new workloads)

5. FSx for openzfs --> Fs compatible with NFS(v3....), move workloads running on zfs to aws, snapshot 				       compression and low-cot,point in time cloning(helpful for testing new workloads)

Data Sync:
==========

=> Data sync is a service used to move large amt of data to and from places.(i.e) move data from on-prem or other cloud location into AWS using (NFS,SMB,S3 API...) and needs agent to run on on-prem

=> It will copy the data and also meta data will be kept b/w diff aws storage service. 

=> Data sync is synchronized but its not a continuouse service it is a scheduled task like hourly,daily,weekly.
   It will preserve meta data and file permission

=> AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services.


AWs storage Gateway
====================

=> AWS storgage gateway is a bridge b/w on-prem and AWS. Used to DR, backup and restore.

Types: 

(i) S3 file gateway --> Configured s3 buckets are accessible using NFS and SMB protocol, supports s3 std, s3 std 			IA, S3 one-zone, s3 intelligent tiring

(ii) FSx file gateway --> Native access to windows file server, useful for grp file shares and home directories

(iii) voulme gateway --> Block storage iscsi protocol backed by s3, Backed by EBS snapshot which can help 				 restore on-prem volumes.
* cached volume: low-latency access to most recent data

* stored volume: entire dataset is on-prem, schdueled backup to s3

(iv) tape gateway --> Virtual tape library(VTL) backed by s3 glacier and s3.

Hybrid cloud --> part of infrastructure is on-prem + cloud

Native cloud storage optons:

Block storage: EBS, Ec2 instance store

File storage: EFS, FSX

Object storage: S3, Glacier

Route 53
========

=> Its a domain register --- 53 is a traditional DNS service. It can check health of the resource.

=> Route 53 supports A/AAAA/CNAME/ NS

A --> maps hostname to ipv4

AAAA--> maps hostname to ipv6

CNAME --> maps hostname into another hostname (but target is a domain which must have A/AAAA record).

NS --> Name servers for the hosted zone( used to control how traffic is routed to domain and its subdomain)

Public HZ: how to route traffic on public internet

Private HZ: how to route traffic within one or more VPCs(private domain name)

=> Each record contains

Domain/subdomain Name: e.g: exmaple.com
Record type: A/AAAA
Value:e.g: 12.53.45.23
Routing policy: how route53 responds to quries
TTL: amt of time the record cached at DNS resolvers

CNAME vs Alias:

CNAME: Points a hostname to any other hostname
Alias: points a hostname to an aws resource

=> CNAME only work for non-root domain but Alias works for both root and non-root domain

=> Alias automatically recognizes changes in the ip address but in CNAMe may be chance of outdated records

=> Alias records always type of A/AAAA coz aws resources have (ipv4/ipv6)

=> CNAME we have to set TTL(Time To Live) for ip changes but in Alias its setup automatically by Route53

=> We cannot set alias record for EC2 DNS

Health check
============

=> Route53 health checkers are outside of vpc. They cant access private endpoints

=> Failover routing policy there can be only one primary and one secondary Ec2 req.

=> Using Geo proximity policy is easy way to shift traffic from one region to another by increasing bias.

Decoupling applications: SQS,SNS,Knesis
========================================

SQS --> Fully managed and it will be used to decouple application, unlimited throughput,unlimited # of messages in queue but each msg is short lived(Default retention of msg is 4 to 14 days)	

=> SQS FIFO (First-In-First-Out) Queues have all the capabilities of the SQS Standard Queue, plus the following two features. First, The order in which messages are sent and received are strictly preserved and a message is delivered once and remains available until a consumer process and deletes it. Second, duplicated messages are not introduced into the queue.

=> 256kb per msg sent, can have duplicate msgs, out of order msg

=> API to send msg to sqs is Send msg. removing duplicates.

=> Default msg visibility timeout is 30sec. If msg is not processed within the visibility timeout it will be processed twice.

=> If visibility timeout is too low, we get duplicates and if visibility timeout is high and consumer crashes re-processing will take time.

=> Consumer could call "Changemsgvisibility" API to get more time and they can edit the visibility so that another consumer will not see that msg.

=> Long polling: when a consumer req msg from the queue, it can optionally "wait" for msges to arrive if there are non in the queue. This is called long polling.

=> When SQS Long Polling is enabled, Amazon SQS reducing the number of empty responses when there are no messages available to return and eliminating false empty responses (when SQS messages are available but aren't included in a response).

=> LP decreases # of API calls. wait time can be 20 sec

=> SQS Visibility Timeout is a period of time during which Amazon SQS prevents other consumers from receiving and processing the message again. In Visibility Timeout, a message is hidden only after it is consumed from the queue. Increasing the Visibility Timeout gives more time to the consumer to process the message and prevent duplicate reading of the message. (default: 30 sec., min.: 0 sec., max.: 12 hours)

=> SQS Dead Letter Queue is where other SQS queues (source queues) can send messages that can't be processed (consumed) successfully. It's useful for debugging as it allows you to isolate problematic messages so you can debug why their processing doesn't succeed.

=> SQS Delay Queues is a period of time during which Amazon SQS keeps new SQS messages invisible to consumers. In SQS Delay Queues, a message is hidden when it is first added to the queue. (default: 0 mins, max.: 15 mins)

SNS: "Event producer" only send msgd to one SNS topic.

=> SNS follows publich/Subscribe mtd. Many aws services can send msg directly to SNS

=> Use SDK to publish topic(create topic,create subscription,publish to topic)

=> only one message is sent to the SNS topic and then "fan-out" to multiple SQS queues. This approach has the following features: it's fully decoupled, no data loss, and you have the ability to add more SQS queues (more applications) over time.

=> SNS only support kinesis datafirehose not data stream.

SNS Security
=============

SNS has same security as SQS.

(i) encryption

=> In-flight encryption using https api, client side encryption if client wants to perform encrypt/decrypt itself.

(ii) Access control: IAM policy to regulate access to SNS API

(iii) SNS Access policy (similar to s3 bucket policy) --> possible to cross-account access to SNS topic

Amazon MQ(message broker)
=========================

=> It doesn't scale as much as SQS/SNS. It runs on server, can run in multi-az with failover

=> It has both queue feature(SQS) and topic(SNS)

Kinesis
========

=> Makes it easy to collect, process and analyse streaming data in real-time.

=> Streaming data is continuoulsy data generated by 1000's of data sources

=> The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity.

Kinesis Data Streams: 

=> streaming service,capture,process and store data streams

=> Scalable and durable real-time streaming service. continuosly capture gigabytes of data per sec from 100s or 1000s of resources

=> write custom code. Real time (~200 ms). Manage scaling(shard splitting/merging)

=> Data storage for 1 to 365 days. support replay capability

=> Kinesis Data Stream uses the partition key associated with each data record to determine which shard a given data record belongs to. When you use the identity of each user as the partition key, this ensures the data for each user is ordered hence sent to the same shard.

Kinesis Data Firehose: 

=> load/ingestion of data streams into 

AWS: Redshift/S3/opensearch
3rd party: splunk/Mongodb/datadog/Newrelic
Custom: send to any http endpoint

=> Easiest way to capture transform and load data streams into aws data stores.

=> Kinesis data firehose knows how to write code. 

=> Automated scaling.No data storage and doesn't support replay capability. 

=> It is near real time. 60sec min latency for non full batches or min 1mb of data at a time. 

Kinesis Data Analytics: 

=> analyse data streams with sql or Apache Flink

=> process data streams to sql or java without having to learn new pgms

=> Kinesis Data Analytics with Kinesis Data Streams as the underlying source of data and it is most appropriate when you want to perform real-time analytics on streams of data

Kinesis video streams: 

=> Capture,process and store video streams

=> Securely stream video from connected devices to aws for analytics,machine learning

Data & Analytics
=================

Athena: Serverless sql query service used to to analyse data stored in S3. 

=> Supports Json,csv,orc,Avro and parquet

=> Usecases: BI, analytics/reporting,vpc flow logs, ELB logs, cloudtrail,etc.. commonly used for amazon quicksight for reporting/dashboard.

=> we can use columnar datatype for less scan(scan columns only you need to) for cost saving. Apache parquet or ORC is recommended

=> Use glue to convert ur data to parquet or ORC

Federated query: 

=>Allows you to run sql query across data stored in relation/non-relation,AWS, or on-prem

=> We can use data source connector to run federated query on AWS lambda

=> Store results in s3 for later analysis	

Redshift Cluster
================

=> Leader Node: For query planning, result aggregation

=> Compute Node: For performing the query and send results to leader.

=> We can use resreved instance for cost saving

=> Redshift snapshots are point-in-time backup of cluster which is stored in s3 and we can restore a snapshot into new cluster.

=> Snapshot is automated for every 8hrs or every 5gb or on scheduled and we can copy snapshot into another region as well.

=> Enhanced VPC feature in Redshift forces all COPY and UNLOAD traffic moving between your cluster and data repositories through your VPCs

Loading data into Redshift:
============================

=> Kinesis Data firehose, s3 using copy cmd, Ec2 instance JDBC driver

EMR - Elastic Map Reduce
=========================

=> Helps to creating Hadoop cluster(Big data) to analyse and process vast amt of  data.

=> EMR bundles with Apache spark,Hbase, pres..and it will take care of provisioning and configurations

Use cases: ML,data processing, web index, big data

Master Node : Manage cluster and helath

Core Node:Run task and store data

Task node(optional): run tasks

Amazon Quicksight
=================

=> Quicksight is a ML-powered BI service to create interactive dashboard

=> Integrated with RDS,Aurora,Athena(To do ad-hoc service),Redshift(Data warehouse),s3(to import data).....

=> Dashboard defines Users(std) and Groups(enterprise version) and its a readonly snapshot of analysis 

=> you can share analysis or dashboard to the user or grp but first you need to publish before share

Glue
=====

=> Glue is a ETL(Extraxt,transform and load) service. Its fully serverless and very useful to transfer data for analyse

=> we can convert parquet format(columnar data fmt). (i.e) put datas ito s3 which is csv fmt then we can use Glue ETL tool to import csv and convert it in parquet fmt.Then we can send it to s3 o/p bucket then Athena will analyse the data

=> To automate above process we can send event notification to lambda function which will trigger glue ETL

=> Glue job bookmarks prevents re-processing of old data

=> Glue Elastic view combine and replicate data across multiple data stores using sql.

=> Glue Databrew does clean and normalize data using pre-built transfomation

=> Glue studio is GUI used to create, run and monitor ETL jobs

Data Lake Formation
====================

=> Data lake is a cnetral place hae all your data and on top it we can do analysis.

=> It automtes many complex manual steps like collect,Cleanse, moving catalog and de-duplicating using ML transform

=> Combine structured and unstructured data in the data lake

=> Fine-grained access ctrl of ur application(row and column level). you want to control access to part of the data as it might contain sensitive information

MSK- Amazon Managed Streaming for Apache Kafka
===============================================

=> Alternative service for Kinesis

=> Allow you to create,update delete clusters and MSK creates & manage kafka brokernodes and zookeeper nodes

=> Automatic recovery from kafka failue

Kafka cluster is made of multiple clusters and it has producers to produce data to ingest from places such as kinesis,iot..and then it will send the data directly to kafka topic. Consumers will pull data from topics to process it or send it to various destinations like EMR,S3,Sagemaker,Kinesis, RDS..

Kinesis data stream vs MSK
============================

Kinesis DS: 1MB size, Data streams with shards, shard splliting and merging, KMS at-rest encryption

MSK: 1mb default configure upto 10MB, Kafka topics with partition,KMS at-rest encryption

Machine Learning
==================

=> Rekognition: Face detection,labelling, celebrity recognition. It can automatically detect inappropriate and offensive images and videos and give you the ability to set a minimum confidence threshold for items that will be flagged 

=> Transcribe: Audio to text also used to remove any Personally Identifiable Information (PII) 

=> Polly: Text to audio. 

=> Polly has 2 features like 

* Pronounciation Lexican --> which allows you to customize the pronunciation of words (e.g., Amazon EC2 will 				     be Amazon Elastic Compute Cloud).

=> Lexicon used to customize the pronounciation of stylish words or acronyms

* SSML(speech synthesis markup language) --> which allows you to emphasize words, including breathing sounds, 						     whispering, and more 

Amazon lex & Connect:

=> Lex is same technology as Alexa. Used to Automatic speech recognition(ASR) to convert speech text,Natural language understanding to recognise the intent of caller,text.

=> Helps to build chatbot, call centre bots

=> Amazon connect used to receive calls, create contact flows cloud based virtual contact centre.

Amazon comprehend:

=> Natural Language processing(NLP) its fully managed and serverless service

=> Uses ML to find insights and relationship in text like language of the text, understand how +ve or -ve the text is, automatically organize text files by topic,extract key phrases, people, places,brand and events

Comprehend Medical: Designed and implemented to keep patients privacy by identifying Protected Health 				    Information (PHI) so the solution will be eligible with HIPAA.

=> Amazon sagemaker: used to create and deploy ML models.  

=> developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models.

=> Amzon forecast: (i.e) used to predict the fulture sales of raincoat, Product demand planning,Financial planning,resource planning

=> Amazon kendra used for document search service and extract docs like pdf,word,zip....

=> Amazon Textact: Autpomatically extract txt, handwriting, data from scanned docs using AI and ML

Usecase: Finacial service(invoice, finacial reports) Healthcare: medical records, insurance name

=> Amazon Personalize: Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications

Amazon macie: Data security and data privacy service for ML and pattern matching to discover and protect senstive data.

=> Helps to identify sensitive info such as personally identifiable information(PII) in s3 buck and notify to eventbridge and sns send notification

Docker (ECS)
================

=> Docker hub : Public repo

=> ECR (Elastic container Registry): Private & Public repo

=> AWS Fargate: Amazon's own Serverless container platform works with both ECS & EKS.

=> AWS Fargate allows you to run your containers on AWS without managing any servers.

=> There are 2 launch types ECS and ECR

=> In ECS you must provision & maintain infrastructure(EC2)

=> Ec2 instance must run the ECS agent to register in the ECS Cluster(AWS launnch ECS tasks on ECS cluster)

=> AWS take care of start/stop the container

ECR: 

=> In ECR you do not provision& maintain infrastructure(EC2). Its all serverless. We just need to create task definition to define our ECS task

=> By default Fargate task spread across az.

=> Behind the scene ECS images stored in S3

=> Support img vulnerability scanning, versioning, img tags. img lifecycle

=> Amazon ECR is a fully managed container registry that makes it easy to store, manage, share, and deploy your container images. It won't help in running your Docker-based applications.

=> ECR is fully integrated with Amazon ECS, allowing easy retrieval of container images from ECR while managing and running containers using ECS.

EFS for Docker(ECS)
===================

=> EFS volume can be shared between different EC2 instances and different ECS Tasks. It can be used as a persistent multi-AZ shared storage for your containers.

IAM Policy for ECS:
=====================

=> We need to create IAM role to perform actions like send container logs to CW, Pull image from ECR, Refer sensitive data in secret mgr or ssm parameter store

=> Task role is defined in Task definition

=> S3 cannot be mounted as a filesystem on ECS task

=> ECS Task Role is the IAM Role used by the ECS task itself. Use when your container wants to call other AWS services like S3, SQS, etc.

ECS-ASG
=========

=> Automatically increase/decreses # of desired tasks.

EKS
====

=> EKS doesn't support AWs Lambda node.

AWS APP Runner
===============

=> allows the developer to build and deploy the website and the APIs in the easiest way. No need to have deep understanding abt AWS services.

Lambda Serverless
==================

=> Serverless means developers don't have to manage the servers

=> Lambda fun is a virtual fun no need to manage servers, used for short execution, Run on-demand and scaling is automated

=> Integrated with many pgm lang like Node.js,C#, Python,Java,Golang,Ruby

=> Easy monitoring through cloud watch

=> Serverless cronjob(need to run on virtual server). Cloudwatch Eventbridge will trigger the cronjob function

=> Lambda's maximum execution time is 15 minutes. In that period if you have timeout error You can run your code somewhere else such as an EC2 instance or use Amazon ECS.

=> RCU and WCU are decoupled, so you can increase/decrease each value separately.

Lambda limits per region
=========================

(i) Execution limit

=> Memory allocation 128mb to 10gb and max exe time 15min(900sec) and concurrency exe 1000(can be increased)

=> Env var 4kb and disk capacity in fun container(in /tmp) 512mb to 10gb

(ii) Deployment limit

=> Deployment size(.zip compressed) 50mb and uncompressed 250mb

Cloudfront function vs Lambda@Edge function
============================================

=> Cloudfront fun: is lightweight mutabel fun written in js. It has high-scale,latency-sensitive CDN cutomization

=> Viewer req: After CF receives a req from viewer

=> Viewer Response: Before CF frwds a response to the viewer

=> Max exe time <1ms

Lambda@Edge fun: wriiten in Nodejs or python, scaless 1000s of req/sec

=> Used to change CF req and response

* Viewer req: After CF receives a req from viewer

* Origin req:Before CF frwds a req to the origin

* Origin response: After CF recives the response from the origin

* Viewer Response: Before CF frwds a response to the viewer

=> Max exe time 5-10 sec

Lambda with RDS Proxy
=======================

=> If lambda fun directly hits RDS there may be many connection issue so that its connecting RDS via proxy and RDS proxy is never publicly accessesd so  lambda funs need to deploy into VPC

=> Security - IAM Authentication, Creds stores in Secret mgnr

=> Lambda funs supports in Postgres, Aurora Mysql, Dynamo DB

=> Dynamo DB HA with replication across multi az. NoSQL DB- Non-relational db and distributed db

=> Dynamo DB integrated with IAM for security, authentication and authorization

=> Dynamo DB std and infrequnet access(IA) tbl class

=> Dynamo DB has 2 modes to do Read/write throughput such as provision mode(defualt), on-demand mode

=> On-demand mode is good for faster transaction and it automatically scale up/down workloads and its good for unpredictable workload

=> RCU and WCU are decoupled, so you can increase/decrease each value separately.

=> DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain or operate. It automatically scales tables up and down to adjust for capacity and maintain performance. It provides both provisioned (specify RCU & WCU) and on-demand (pay for what you use) capacity modes.

DynamoDB Accelerator(DAX)
==========================

=> DAX is frontend of dynamodb

=> microsec latency for cached data and default TTL is 5min

=> DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to 10x performance improvement. It caches the most frequently used data, thus offloading the heavy reads on hot keys off your DynamoDB table, hence preventing the "ProvisionedThroughputExceededException" exception.

Dynamo DB stream vs kinesis stream
=====================================

=> DDB has 24hrs retention and limited # consumer access

=> Kinesis has 1yr retention with high # of consumer acess

Dynamo DB Global table
========================

=> DDB is a global tbl and going to replicate across multiple regions

=> Active-Active replication means app can read/write to the tbl in any region

Dynamodb backup
================

(i) Continuous bkup using point in time recovery(PITR)

=> PITR to anytime within the bkup window, enbaled for last 35 days

=->recovry process creats new tbl

(ii) on-demand bkup

=> long-term retention until explicitely deleted

=> Doesn't affect perf or latency

Dynamodb integration with s3
==============================

=> EXport to s3 must enable PITR

Amazon cognito
===============

=> Cognito and step fun is used for mobile users

Distaster Recovery
===================

=> RPO --> Recovery point objective --> During DR defines how much of data loss willing to accept
=> RTO --> Recovery Time objective --> Defines application downtime

DR stategy
==========

(i) bkup and restore
(ii) Pilot light --> Useful for critical core, Faster than bkup and restore
(iii) warm standby --> has a potentially high Recovery Point Objective (RPO) and Recovery Time Objective (RTO)
(iv) Hotsite/ Multisite  application --> very low RTO(mins/secs) but very expensive

=> For HA use route 53 to migrate DNS over from region to region, site to site vpn as a recovery from direct connect	

Data migration service
=========================

=> Support homogeneous: Ex: oracle to orcle.

=> Support hetrogeneous: Ex: mysql to orcle

=> Continuous data replication and we must create ec2 instance to perform replication task

Application Discovery service:

=> Before migrate app need to collect all the infos abt on-prem datacentres. In that Application Discovery service will help us. It will scan the servers and gather all infos like server utilization and dependency mapping which is very important to migration

* Agentless Discovery --> used to collect performance history such as cpu,memory and disk usage , VM inventory

* Agentbased Discovery --> Gives more info abt VMs such as s/y configuration, performance, processor are running 
			   and detail n/w connection

Application Migration service(MGN)--> Lift and shift, used to simplify migration process

AWS Schema conversion tool
==========================

=> Convert ur db schema from one engine to another and no need to use SCT if you are migrating the same DB engine (i.e) on-prem psql to RDS psql

Compute & Networking
====================

=> ENA(Elastic N/w Adapter) used to enhance EC2 n/w bandwidth with lower latency. It delivers n/w speed as 100gpbs

=> EFA Elastic Fabric Adapter only works in linux and it will improve ENA and High performance, tightly coupled workloads

=> Msg passing Interface(MPI) used to bypass os to provide reliable n/w ,transport and low latnecy

=> AWS Batch support multi-node pallel jobs helps to run single jobs that span multiple Ec2 instances and eaily shedule jobs.

Cloudwatch
============

=> CW provides metrics for all services in aws, metrics belong to namespaces and it has timestamp

=> cw is a query engine not a real time engine

=> Log data can take upto 12hrs to export

=> Send log events in multiple account

=> need to create cw agent to collect and push the logds to cw dashboard with help of correct IAM policy
   and cw agent can setup on-prem too.

=> Type of cw logs

* CW log agent --> old version can only send CW logs

* CW unified agent --> Collect additional s/y level metrics such as RAM, processes,etc also colect logs and send it to CW

=> CW is a single metric if you wnat to set multiple metrics need to set composite alarm

=> CW event bridge is known as CW events. with this we can schedule cron jobs

=> Eventrules can react if service doing something. AWS events can be stored in default event bus and we can archive events sent to an event bus and we can replay archieve events

Cloud trail
==============

=> Cloud trial is governance,compliance and audit for aws account

=> CT insight is used to detect unusual activity in your AWS Account

=> If resource is deleted we can investigate it in cloudtrail

=> Event stored 90days in cloudtrail, to keep events beyond this period ,log them to s3 and use Athena

=> To enable cloudtrail insights we can track unusual activites in aws account

CW vs CT vs Config
==================

=> CW is fpr perf monitoring(metric,cpu,n/w) and dashboard, event & alert, log aggregation & Analysis

=> CT is for Record API calls mades within ur account by everyone, Global service, Can define trail for specific resource

=> Config is for track and record configuration changes, Evaluate resources against complaince rules, Get timeline of changes and compliances

=. Also config will ensure SSL cert is assinged to LB

=> AWS Config remediation is used to monitor Security Groups if there's unrestricted SSH access to any of your EC2 instances. Which AWS Config feature is used to automatically re-configure your Security Groups to their correct state

VPC
====

=> We can have multiple vpcs in aws region max 5 per region) and max cidr per vpc is 5

=>Since vpc is private ip4 only allowed and vpc cidr should not overlapped with other n/w

=> AWS reservs 5 IP addresses

=> If you need 29 ip address for ec2 you cant choose subnet size /27 (i.e)(32 ips (32-5 =27 <29) )instead you can chooe 26 (64 ips 64-5 =59 >29)

=> CIDR not should overlap, and the max CIDR size in AWS is /16.

IGW
===

=> IGW scales horizontally, must be created seperately from VPC and one vpc can attached to 1 IGW and vise versa

=> IGW on their own do not allow internet access , route tbls must be associated

Bastion host
=============

=> Bastion host is a EC2 instqance it is spl coz it is in public subnet and it has own sg called bastion host sg

=> Ec2 instance(bastion host) in public subnet can access Ec2 instance in private subnet via ssh. SG of instance in private subnet must allow bastion host SG or private ip of bastion host

NAT(Network Access Translator) Instance
========================================

=> Allow ec2 instances in private subent to connect to the internet and Nat instance must be launched in public subnet with attached eip.

=> Some ips are rewritten so need to disable source and destination check in NAT.

=> Internet bandwidht is based on EC2 instanc. In NAT instance we must manage sg & rules

=> For NAT instance NAT AMIs are available

NAT Gateway
===========

=> NAT Gateway is AWS managed, higher bandwidht, HA(neet to setup multiple NAT GW connection). It will create specific az use an eip

=. NAT GW cannot work without IGW and also can't be used with same subnets

=> No SG requires. 1 Nat Gw is resilent for 1 az. For fault tolerance we need to create multiple NAT GW for multi az

=> one az goes down all the instances within the az is not accessible

NACL
=====

=> NACL is stateless and SG is stateful. NACL controls trffic in subnet level and one NACL per subnet

=> NACL defaultly allow/deny rules we need to make it accessible

=> Default NACL accept everything inbound/outbound with subnet associated with

VPC Peering
=============

=> Connection b/w 2 vpcs is called vpc peering. Must not have overlapping CIDR.

=> Must update route tbl for each vpc subnet and to ensure ec2 instances can communicate each other.

=> Can create vpc peering in same accouunt/cross act

VPC Endpoint
============

=> Using endpoint we can contact other aws services without public internet without help of IGW and NAT GW

* Interface Endpoint:

=> Provisions an ENI (private ip) as an entry point must attach SG and supports most AWS services

=> Interface endpoint is preferred access is required from on-prem(site to site vpn or direct connect)

* Gateway Endpoint:

=> Provisions a gateway and must be used as a target in a rt tbl (does not use SG)

=> Supports both s3 nd dynamo db

=> Free and scales more

=> Need to Enable ICMP protocol to access ping cmd

VPC Flow logs
==============

=> Allows you to capture info abt ip traffic going into interfaces like vpc flow logs,subnet flow logs, Elastic Network interface(ENI) flow logs and helps to monitor and troubleshoot connectivity issue

=> logs can go to s3, cw logs,kinesis data firehose

=> Flowlog meta datas like 

srcaddr & dstaddr --> Helps to identify problematic ip
srcport & dstport --> Helps to identify problematic port
Action --> Success/failure of the req due to sg/NACL

=> Also used to find malicious behaviour

Site-to-site VPN
=================

=> If you want to connect AWS to corporate datacentre using a private connection we can do tat via customer gateway on the corporation side and we have VPN gateway on the vpc side. So we can estabilish tis through Private site-to-site connection. It is encrypted

* Virtual private Gateway	

=> VGW is created and attached to the vpc from which you want to create the site-to-site vpn connection

* Customer gateway

=> s/w application or physical device on customer side of the vpn connection	 

=> site-to-site vpn connection will not work until enable route propagation in vpc within the subnet

=> Need to Enable ICMP protocol to access ping cmd

VPN cloudhub
============

=> When we have multiple customer n/w and each has its own CGW, so cloudhub provides secure communication b/w all of these sites using multiple vpn connection. Using this customer can communicate with other vpn connection

=> Its connected via publiuc internet but vpn connection is encrypted

=> AWS VPN CloudHub allows you to securely communicate with multiple sites using AWS VPN. It operates on a simple hub-and-spoke model that you can use with or without a VPC.

Direct connect(DX)
==================

=> It provides dedicated private connection from a remote n/w to ur vpc direct connection must be setup b/w DC and AWS direct connect location

=> To setup either dedicated or hosted connection lead times are often longer than 1 month to establish a new connection

=> Using a Direct Connect connection, you can access both public and private AWS resources.

=> High resiliency for critical workload. Where we setup multiple DC we can have 2 corporate datacentres and two diff DC location. So one DC location goes down we have bkup DC location somwhere else

=> Main use of DX is have set up a Direct Connect connection between your corporate data center and your VPC A in your AWS account. You need to access VPC B in another AWS region from your corporate datacenter as well.

=> A Dedicated Direct Connect connection supports 1Gbps to 10Gbps.

=> Hosted Direct Connect connection supports 50Mbps, 500Mbps, up to 10Gbps.

Egress-only IGW
================

=> Used only for IPV6 similar to NAT GW

AWS Network Firewall
=====================

=> Used to protect entire vpc and we can inspect in any direction like vpc to vpc, outbound to internet, inbound fro internet, To/From site-to-site vpn connection , Direct connect

=> Fine grind controls  supports 1000s of rule, send logs to CW,s3, kinesis

=> Traffic filtering: Allow,drop or alert for the traffic that matched with rules

Transit Gateway
=================

=> Transitive peering connection for vpc and on-prem,VPN and DX

=> AWS Transit Gateway is a service that connects VPCs and on-premises networks through a central hub1. 

=> It simplifies your network and reduces the complexity of peering relationships1. It acts as a cloud router that can scale globally1

Networking cost
================

=> Use private ip instead public ip for cost saving and better performance. Also use same az for cost saving and for HA use read replica in same az

=> S3 to Cloudfront transaction is free

Security KMS,SSM,Sheid
========================

Encyption: TLS cert helps with encryption. Encryption inflight ensures no man in middle attack(mima)

=> To enable In-flight Encryption (In-Transit Encryption), we need to have HTTPS endpoint with SSL cert

=> In server side encryption we will share the data key to the server to decrypt but in client side encryption/decryption server will not provide any key all have to tack care by client side

=> Server-Side Encryption means the server will encrypt the data for us. We don't need to encrypt it beforehand.

=> In Server-Side Encryption,we can't do encryption/decryption ourselves as we don't have access to the corresponding encryption key both happen on the server

=> Encryption is done by KMS. Fully integrated with IAM authorization

=> In cloud trial need to enable KMS audit log

=> AWS owned keys(free) --> SSE-s3, SSE-SQS, SSE-DDB(default), customer managed keys(1$ per month)

=> Automatic key rotation --> automatic renew every 1 year

=> Same KMS key cannot live in two regions

=> Secrete Mgnr is most suitable AWS service for storing RDS DB passwords which also provides you automatic rotation

Key types
=========

(i) Symmetric key

=> single key which is used for both encryption/decryption

=> To decrypt we must call KMS API

(ii) Assymetric key

=> We two keys like public(Encrypt) and privatekey(Decrypt)

=> Public key is downloadable and in private key we cannot use outside of AWS, only accessible within the account

KMS Key policy
===============

=> Same as S3 bucket policy and the diff is cannot control them without access

(i) Defualt KMS key policy 

=> Created defaultly if you create it and has access to the root user

(ii) Custom kms key

=> Usefulfor cross-access account

=> It defines Users, Roles that can acces the access the key and define who can adminster the key

=> Used for copying snapshot on cross account and create volume from snapshot

Multi-region key
=================

=> Identical keys in diff region can interchangable

=> Multi-region keys have same-id, key material and automatic rotation and we can encrypt in one region and decrypt in other region. 

=> No-need to de-encrypt or making cross-region API Calls.

=> Multi-region key is not global we need to manage it independantly 

S3 Replication encryption
==========================

=> Unencrypted objs and encrypted objs in sse-s3 are replicated by default

=> objs encrypted with SSE-C(customer provide key) can be replicated

=> objs encrypted with SSE-KMS will not replicated by defualt we need to enable option

=> we can use multi-region kms key for s3 replication but currently they are treated as independant so it will be decrpted first and then re-encrypt

SSM Parameter
=============

=> Secure storage for configuration and secrets. Security through IAM and get notifications through Eventbridge

=> We will stores screts in Secret Manager and it has capability to force rotation in every x days and it will automate this rotation using lambda

=> Replicate secrets across multiple regions with encrypted.

ACM-AWS Certificate Manager
==============================

=> Provide in-flight encryption for https

=> Supports both public & private TLS certs

=> Cannot use ACM with EC2 (it can be extracted)

=> We can generate cert outside of ACM and import it but no automatic renewal

=> To integrate ACM with API need to create a custom domain name in API gateway

* Edge optimized Endpoint: For Global clients

=> Reqs are routed through the CF Edge location(improves latnecy) and the API gateway lives in only one region

* Regional --> Clients within the same region

* Private endpoint 

=> Can only be accessed from VPC using an interface vpc endpoint

=> Use resource policy to define access

=> As the Edge-Optimized API Gateway is using a custom AWS managed CloudFront distribution behind the scene to route requests across the globe through CloudFront Edge locations, the ACM certificate must be created in us-east-1.

WAF(Web Application Firewall)
=============================

=> Protects web application from common web exploits

=> It only deploys ALB, API Gsateway, CF, Appsync Graphsql, Cognito user pool and its not possible to NLB

=> Once we deployed services need to define web ACL rules. Ip set up upto 10000, if you need more rules we need to set up more ips.

=> Web ACL is regional except CF. 

=> ALB doesn't have fixed ip. To fix this we can use global accelarator for fixed ip

=> WAF Manager:If you want to use WAF across act, automate the protection of new resources, accelarate new WAF configuration use WAF mgnr.


AWS Shield: Protect from DDOS
==============================

=> DDOS(Distributed Deniel of service)--> many reqs at the same time to protect this we have aws sheild service and its a free service

=> Optional ddos mitigation service is the advanced service to provide extra sheild and its a chargeble. It provides 24/7 access to AWS DDOS response team

Amazon Guarduty
================

=> Intelligent threat discovery to protect aws account

=> AWS GuardDuty scans the data sources such as vpc flowlogs, cloudtrail logs, DNS logs and it will not scan CW logs

Amazon inspecter --> automated security assesment

=> Analyse the vulnerabilities for ec2 instances, assesment for container images and for lambda function identifies s/w vulnerability and pkg dependency.

=> send findings to eventbridge. 

=> continuouse scanning of the infra when you need

Amazon Pinpoint
================

=> Its a scalable 2-way(inbound/outbound) marketing service. Supports email,sms,push,voice and in-app msg

=> Scales to billions of msg per day and possibilities to receive reply

=> SNS and SES msging servce,delivery schedule but pinpoint create msg template, delivery schedule,targeted segments and fully campaign

System manager and SSM session manager
=======================================

=> Allows you to start a ssh on ur ece and on-prem servers without having access of ssh,bastion host, or ssh keys, No port 22 required but we need IAM role access

=> send session logs to s3 or cw

=> fleet mgnr all the instances are registered with ssm

SSM manager maintenance window: Defines a schedule for when to perform action on ur instance like os patching,updating drivers,installing s/w

SSM mgnr automation runbook ssm documents to defines action perform to ur instance

Trusted Advisor
===============

* Cost optimization, performance, security, fault tolerance, service limits

7 core checks: Basic & developer support plan

S3 bkt permission, SG(specific port unrestricted), IAM use(1 iam user min), MFA on root account, EBS public snapsot,RDS public snapsot, service limits

Full checks: Business & enterprise support plan

* Programatic access using aws support api






























 d





































































































          

